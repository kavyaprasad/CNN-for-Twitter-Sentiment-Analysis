# -*- coding: utf-8 -*-
"""Sentiment Analysis using CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VWXC3A37H0Kxh8Er5dSea184BS-jZiK9
"""

#string matching
import re 

#reading files
import pandas as pd

#handling html data
from bs4 import BeautifulSoup

#visualization
import matplotlib.pyplot as plt  

pd.set_option('display.max_colwidth', 200)

from google.colab import drive
drive.mount('/content/drive')

# load the stackoverflow questions dataset
train_df = pd.read_csv('/content/drive/My Drive/Sentiment Analysis/train_2kmZucJ.csv')

train_df

# Text Cleaning
#Let's define a function to clean the text data.

def cleaner(text):

  text = BeautifulSoup(text).get_text()
  
  # fetch alphabetic characters
  text = re.sub('http[s]?://\S+', '', text) 
  #print(text)
  text = re.sub("[^a-zA-Z]", " ", text)

  # convert text to lower case
  text = text.lower()

  # split text into tokens to remove whitespaces
  tokens = text.split()

  return " ".join(tokens)

# call preprocessing function
train_df['cleaned_text'] = train_df['tweet'].apply(cleaner)

train_df['tweet'][1]

train_df['cleaned_text'][1]

train_df['label'].value_counts()

train_df.head(105)

x = train_df['cleaned_text']
x.head(10)

y =  train_df['label']
y.head(10)

from sklearn.model_selection import train_test_split
x_tr,x_val,y_tr,y_val=train_test_split(x, y, test_size=0.2, random_state=0,shuffle=True)

# Text Representation

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences 

#prepare a tokenizer
x_tokenizer = Tokenizer() 

#prepare vocabulary
x_tokenizer.fit_on_texts(x_tr)

len(x_tokenizer.word_index)

thresh = 3

cnt=0
for key,value in x_tokenizer.word_counts.items():
  if value>=thresh:
    cnt=cnt+1

print(cnt)

# prepare the tokenizer again
x_tokenizer = Tokenizer(num_words=cnt,oov_token='unk')

#prepare vocabulary
x_tokenizer.fit_on_texts(x_tr)

#define threshold for maximum length of a setence
max_len=100
#convert text sequences into integer sequences
x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) 
x_val_seq = x_tokenizer.texts_to_sequences(x_val)

#padding up with zero 
x_tr_seq = pad_sequences(x_tr_seq,  padding='post', maxlen=max_len)
x_val_seq = pad_sequences(x_val_seq, padding='post', maxlen=max_len)

#no. of unique words
x_voc_size = x_tokenizer.num_words + 1
x_voc_size

x_tr_seq[20]

# Model Building

from keras.models import *
from keras.layers import *
from keras.callbacks import *
import keras.backend as k

# define model architecture
k.clear_session()
model =  Sequential()
model.add(Embedding(x_voc_size, 50, trainable=True, input_shape=(max_len,)))  #embedding layer
  
model.add(Conv1D(64,3,padding='same'))  #conv1d layer
model.add(Dropout(0.1))

model.add(GlobalMaxPooling1D()) 
  
model.add(Dense(128,activation='relu'))  #dense layer

model.add(Dense(1 ,activation='sigmoid')) #output layer
model.summary() #summary) of model

model.summary()

import keras
#define optimizer and loss
optimizer = keras.optimizers.Adam(lr=0.00005)

model.compile(optimizer=optimizer,loss='binary_crossentropy')

#checkpoint to save best model during training
mc = ModelCheckpoint("weights.best.hdf5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

#train the model 
model.fit(x_tr_seq, y_tr, batch_size=128, epochs=100, verbose=1, validation_data=(x_val_seq, y_val), callbacks=[mc])

# Model Predictions 
# load weights into new model
model.load_weights("weights.best.hdf5")

#predict probabilities
pred_prob = model.predict(x_val_seq)

pred_prob[3]

import numpy as np
#define candidate threshold values

threshold  = np.arange(0,0.5,0.01)
threshold

# convert probabilities into classes or tags based on a threshold value
def classify(pred_prob,thresh):
  y_pred_seq = []

  for i in pred_prob:
    temp=[]
    for j in i:
      if j>=thresh:
        temp.append(1)
      else:
        temp.append(0)
    y_pred_seq.append(temp)

  return y_pred_seq

from sklearn import metrics
score=[]

#convert to 1 array
y_true = np.array(y_val).ravel() 

for thresh in threshold:
    
    #classes for each threshold
    y_pred_seq = classify(pred_prob,thresh) 

    #convert to 1d array
    y_pred = np.array(y_pred_seq).ravel()

    score.append(metrics.f1_score(y_true,y_pred))

# find the optimal threshold
opt = threshold[score.index(max(score))]
opt

#predictions for optimal threshold
y_pred_seq = classify(pred_prob,opt)
y_pred = np.array(y_pred_seq).ravel()

print(metrics.classification_report(y_true,y_pred))

# load the stackoverflow questions dataset
test_df = pd.read_csv('/content/drive/My Drive/Sentiment Analysis/test_12QyDcx.csv')

test_df

# Text Cleaning
#Let's define a function to clean the text data.

def cleaner(text):

  text = BeautifulSoup(text).get_text()
  
  # fetch alphabetic characters
  text = re.sub('http[s]?://\S+', '', text) 
  text = re.sub("[^a-zA-Z]", " ", text)

  # convert text to lower case
  text = text.lower()

  # split text into tokens to remove whitespaces
  tokens = text.split()

  return " ".join(tokens)

# call preprocessing function
test_df['cleaned_text'] = test_df['tweet'].apply(cleaner)

test_df.head(10)

x_test = test_df['cleaned_text']
x_test

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences 

#prepare a tokenizer
x_tokenizer = Tokenizer() 

#prepare vocabulary
x_tokenizer.fit_on_texts(x_test)

len(x_tokenizer.word_index)

thresh = 3

cnt=0
for key,value in x_tokenizer.word_counts.items():
  if value>=thresh:
    cnt=cnt+1

print(cnt)

# prepare the tokenizer again
x_tokenizer = Tokenizer(num_words=cnt,oov_token='unk')

#prepare vocabulary
x_tokenizer.fit_on_texts(x_test)

#define threshold for maximum length of a setence
max_len=100
#convert text sequences into integer sequences
x_test_seq = x_tokenizer.texts_to_sequences(x_test) 
#x_val_seq = x_tokenizer.texts_to_sequences(x_val)

#padding up with zero 
x_test_seq = pad_sequences(x_test_seq,  padding='post', maxlen=max_len)
#x_val_seq = pad_sequences(x_val_seq, padding='post', maxlen=max_len)
x_voc_size = x_tokenizer.num_words + 1
x_voc_size

# Model Predictions 
# load weights into new model
model.load_weights("weights.best.hdf5")

#predict probabilities
pred_prob = model.predict(x_test_seq)
x_test_seq

pred_prob[0]

#predictions for optimal threshold
y_pred_seq = classify(pred_prob,opt)
y_pred = np.array(y_pred_seq).ravel()

y_pred[1]

test_df['label'] = pd.DataFrame(np.array(classify(pred_prob,opt)).ravel())
test_df

test_df=test_df[['id','label']]
test_df

test_df.to_csv("output_sentiment_analysis.csv",index=False)
